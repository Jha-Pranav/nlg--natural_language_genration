{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "import utils \n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/actors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nlp(\"James Stewart is the actor with the highest Rating.\")\n",
    "args = {'_sort': ['-rating']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _search(text, args, df, copy=False):\n",
    "    \"\"\"Construct a tornado template which regenerates some\n",
    "    text from a dataframe and formhandler arguments.\n",
    "\n",
    "    The pipeline consists of:\n",
    "    1. cleaning the text and the dataframe\n",
    "    2. searching the dataframe and FH args for tokens in the text\n",
    "    3. detecting inflections on the tokens.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : spacy.Doc\n",
    "        Input text\n",
    "    args : dict\n",
    "        Formhandler arguments\n",
    "    df : pd.DataFrame\n",
    "        Source dataframe.\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    tuple\n",
    "        of search results, cleaned text and token inflections. The webapp uses\n",
    "        these to construct a tornado template.\n",
    "    \"\"\"\n",
    "    # utils.load_spacy_model()\n",
    "    if copy:\n",
    "        df = df.copy()\n",
    "    df = utils.gfilter(df, args.copy())\n",
    "    # Do this only if needed:\n",
    "    # clean_text = utils.sanitize_text(text.text)\n",
    "    args = utils.sanitize_fh_args(args, df)\n",
    "    # Is this correct?\n",
    "    dfs = DFSearch(df)\n",
    "    dfix = dfs.search(text)\n",
    "    dfix.update(search_args(dfs.ents, args))\n",
    "    dfix.clean()\n",
    "    inflections = grammar.find_inflections(dfix, args, df)\n",
    "    _infl = {}\n",
    "    for token, funcs in inflections.items():\n",
    "        _infl[token] = []\n",
    "        for func in funcs:\n",
    "            _infl[token].append({\n",
    "                'source': func.source,\n",
    "                'fe_name': func.fe_name,\n",
    "                'func_name': func.__name__\n",
    "            })\n",
    "    # FIXME: Why return text if it's unchanged?\n",
    "    return dfix, text, _infl\n",
    "\n",
    "def _df_maxlen(df):\n",
    "    # Find the length of the longest string present in the columns, indices or values of a df\n",
    "    col_max = max([len(c) for c in df.columns.astype(str)])\n",
    "    ix_max = max([len(c) for c in df.index.astype(str)])\n",
    "    array_max = max([df[c].astype(str).apply(len).max() for c in df])\n",
    "    return max(col_max, ix_max, array_max)\n",
    "\n",
    "def search_args(entities, args, lemmatized=True, fmt='fh_args[\"{}\"][{}]',\n",
    "                argkeys=('_sort', '_by', '_c')):\n",
    "    \"\"\"\n",
    "    Search formhandler arguments provided as URL query parameters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    entities : list\n",
    "        list of named entities found in the source text\n",
    "    args : dict\n",
    "        FormHandler args as parsed by g1.url.parse(...).searchList\n",
    "    lemmatized : bool, optional\n",
    "        whether to search on lemmas of text values\n",
    "    fmt : str, optional\n",
    "        String format used to describe FormHandler arguments in the template\n",
    "    argkeys : list, optional\n",
    "        Formhandler argument keys to be considered for the search. Any key not\n",
    "        present in this will be ignored.\n",
    "        # TODO: Column names can be keys too!!\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Mapping of entities / tokens to objects describing where they are found\n",
    "        in Formhandler arguemnts. Each search result object has the following\n",
    "        structure:\n",
    "        {\n",
    "            'type': 'some token',\n",
    "            'location': 'fh_args',\n",
    "            'tmpl': 'fh_args['_by'][0]'  # The template that gets this token from fh_args\n",
    "        }\n",
    "    \"\"\"\n",
    "    args = {k: v for k, v in args.items() if k in argkeys}\n",
    "    search_res = {}\n",
    "    entities = list(chain(*entities))\n",
    "    search_res.update(_search_groupby(entities, args, lemmatized=lemmatized))\n",
    "    search_res.update(_search_sort(entities, args, lemmatized=lemmatized))\n",
    "    search_res.update(_search_select(entities, args, lemmatized=lemmatized))\n",
    "    return search_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFSearch(object):\n",
    "    \"\"\"Make a dataframe searchable.\"\"\"\n",
    "\n",
    "    def __init__(self, df, nlp=None, **kwargs):\n",
    "        \"\"\"Default constructor.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pd.DataFrame\n",
    "            The dataframe to search.\n",
    "        nlp : A `spacy.lang` model, optional\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        # What do results contain?\n",
    "        # A map of tokens to list of search results.\n",
    "        self.results = DFSearchResults()\n",
    "        if not nlp:\n",
    "            nlp = utils.load_spacy_model()\n",
    "        self.matcher = kwargs.get('matcher', utils.make_np_matcher(nlp))\n",
    "        self.ents = []\n",
    "        \n",
    "\n",
    "    def search(self, text, colname_fmt='df.columns[{}]',\n",
    "               cell_fmt='df[\"{}\"].iloc[{}]', **kwargs):\n",
    "        \"\"\"\n",
    "        Search the dataframe.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        text : spacy.Doc\n",
    "            The text to search.\n",
    "        colname_fmt : str, optional\n",
    "            String format to describe dataframe columns in the search results,\n",
    "            can be one of 'df.columns[{}]' or 'df[{}]'.\n",
    "        cell_fmt : str, optional\n",
    "            String format to describe dataframe values in the search results.\n",
    "            Can be one of 'df.iloc[{}, {}]', 'df.loc[{}, {}]', 'df[{}][{}]', etc.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            A dictionary who's keys are tokens from `text` found in\n",
    "            the source dataframe, and values are a list of locations in the df\n",
    "            where they are found.\n",
    "        \"\"\"\n",
    "        self.search_nes(text)\n",
    "        if len(text.text) <= _df_maxlen(self.df):\n",
    "            for i in _text_search_array(text.text, self.df.columns):\n",
    "                self.results[text] = {'location': 'colname', 'tmpl': colname_fmt.format(i),\n",
    "                                      'type': 'doc'}\n",
    "            for x, y in zip(*_text_search_array(text.text, self.df)):\n",
    "                x = utils.sanitize_indices(self.df.shape, x, 0)\n",
    "                y = utils.sanitize_indices(self.df.shape, y, 1)\n",
    "                self.results[text] = {\n",
    "                    'location': 'cell', 'tmpl': cell_fmt.format(self.df.columns[y], x),\n",
    "                    'type': 'doc'}\n",
    "\n",
    "        else:\n",
    "            for token, ix in self.search_columns(text, **kwargs).items():\n",
    "                ix = utils.sanitize_indices(self.df.shape, ix, 1)\n",
    "                self.results[token] = {'location': 'colname', 'tmpl': colname_fmt.format(ix),\n",
    "                                       'type': 'token'}\n",
    "\n",
    "            for token, (x, y) in self.search_table(text, **kwargs).items():\n",
    "                x = utils.sanitize_indices(self.df.shape, x, 0)\n",
    "                y = utils.sanitize_indices(self.df.shape, y, 1)\n",
    "                self.results[token] = {\n",
    "                    'location': 'cell', 'tmpl': cell_fmt.format(self.df.columns[y], x),\n",
    "                    'type': 'token'}\n",
    "            self.search_quant([c for c in text if c.pos_ == 'NUM'])\n",
    "        # self.search_derived_quant([c.text for c in selfdoc if c.pos_ == 'NUM'])\n",
    "\n",
    "        return self.results\n",
    "\n",
    "    def search_nes(self, doc, colname_fmt='df.columns[{}]', cell_fmt='df[\"{}\"].iloc[{}]'):\n",
    "        \"\"\"Find named entities in text, and search for them in the dataframe.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        text : str\n",
    "            The text to search.\n",
    "        \"\"\"\n",
    "        self.ents = utils.ner(doc, self.matcher)\n",
    "        print(\"entities:\",self.ents)\n",
    "        for token, ix in self.search_columns(self.ents, literal=True).items():\n",
    "            ix = utils.sanitize_indices(self.df.shape, ix, 1)\n",
    "            self.results[token] = {\n",
    "                'location': 'colname',\n",
    "                'tmpl': colname_fmt.format(ix), 'type': 'ne'\n",
    "            }\n",
    "        for token, (x, y) in self.search_table(self.ents, literal=True).items():\n",
    "            x = utils.sanitize_indices(self.df.shape, x, 0)\n",
    "            y = utils.sanitize_indices(self.df.shape, y, 1)\n",
    "            self.results[token] = {\n",
    "                'location': 'cell',\n",
    "                'tmpl': cell_fmt.format(self.df.columns[y], x), 'type': 'ne'}\n",
    "\n",
    "    def search_table(self, text, **kwargs):\n",
    "        \"\"\"Search the `.values` attribute of the dataframe for tokens in `text`.\"\"\"\n",
    "        kwargs['array'] = self.df.copy()\n",
    "        return self._search_array(text, **kwargs)\n",
    "\n",
    "    def search_columns(self, text, **kwargs):\n",
    "        \"\"\"Search df columns for tokens in `text`.\"\"\"\n",
    "        kwargs['array'] = self.df.columns\n",
    "        return self._search_array(text, **kwargs)\n",
    "\n",
    "    def search_quant(self, quants, nround=2, cell_fmt='df[\"{}\"].iloc[{}]'):\n",
    "        \"\"\"Search the dataframe for a set of quantitative values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        quants : list / array like\n",
    "            The values to search.\n",
    "        nround : int, optional\n",
    "            Numeric values in the dataframe are rounded to these many\n",
    "            significant digits before searching.\n",
    "        \"\"\"\n",
    "        dfclean = utils.sanitize_df(self.df, nround)\n",
    "        qarray = np.array([c.text for c in quants])\n",
    "        quants = np.array(quants)\n",
    "        n_quant = qarray.astype('float').round(nround)\n",
    "        for x, y in zip(*dfclean.isin(n_quant).values.nonzero()):\n",
    "            x = utils.sanitize_indices(dfclean.shape, x, 0)\n",
    "            y = utils.sanitize_indices(dfclean.shape, y, 1)\n",
    "            tk = quants[n_quant == dfclean.iloc[x, y]][0]\n",
    "            self.results[tk] = {\n",
    "                'location': 'cell', 'tmpl': cell_fmt.format(self.df.columns[y], x),\n",
    "                'type': 'quant'}\n",
    "\n",
    "    def search_derived_quant(self, quants, nround=2):\n",
    "        \"\"\"Search the common derived dataframe parameters for a set of quantitative values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        quants : list / array like\n",
    "            The values to search.\n",
    "        nround : int, optional\n",
    "            Numeric values in the dataframe are rounded to these many\n",
    "            significant digits before searching.\n",
    "        \"\"\"\n",
    "        dfclean = utils.sanitize_df(self.df, nround)\n",
    "        quants = np.array(quants)\n",
    "        #  n_quant = quants.astype('float').round(2)\n",
    "\n",
    "        for num in quants:\n",
    "            if int(num) == len(dfclean):\n",
    "                self.results[num] = {\n",
    "                    'location': 'cell', 'tmpl': \"len(df)\",\n",
    "                    'type': 'quant'}\n",
    "\n",
    "    def _search_array(self, text, array, literal=False,\n",
    "                      case=False, lemmatize=True, nround=False):\n",
    "        \"\"\"Search for tokens in text within an array.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        text : str or spacy document\n",
    "            Text to search\n",
    "        array : array-like\n",
    "            Array to search in.\n",
    "        literal : bool, optional\n",
    "            Whether to match tokens to values literally.\n",
    "        case : bool, optional\n",
    "            If true, run a case sensitive search.\n",
    "        lemmatize : bool, optional\n",
    "            If true (default), search on lemmas of tokens and values.\n",
    "        nround : int, optional\n",
    "            Significant digits used to round `array` before searching.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Mapping of tokens to a sequence of indices within `array`.\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> _search_array('3', np.arange(5))\n",
    "        {'3': [3]}\n",
    "        >>> df = pd.DataFrame(np.eye(3), columns='one punch man'.split())\n",
    "        >>> _search_array('1', df.values)\n",
    "        {'1': [(0, 0), (1, 1), (2, 2)]}\n",
    "        >>> _search_array('punched man', df.columns)\n",
    "        {'punched': [1], 'man': [2]}\n",
    "        >>> _search_array('1 2 buckle my shoe', df.index)\n",
    "        {'1': [1], '2': [2]}\n",
    "        \"\"\"\n",
    "        if array.ndim == 1:\n",
    "            func = _search_1d_array\n",
    "        else:\n",
    "            func = _search_2d_array\n",
    "        return func(text, array, literal, case, lemmatize, nround)\n",
    "        # if len(res) == 0:  # Fall back on searching the whole string, not just the entities\n",
    "        #     res = func([text], array, literal, case, lemmatize, nround)\n",
    "        # return res\n",
    "class DFSearchResults(dict):\n",
    "    \"\"\"A convenience wrapper around `dict` to collect search results.\n",
    "\n",
    "    Different from `dict` in that values are always lists, and setting to\n",
    "    existing key appends to the list.\n",
    "    \"\"\"\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        if key not in self:\n",
    "            super(DFSearchResults, self).__setitem__(key, [value])\n",
    "        elif self[key][0] != value:\n",
    "            self[key].append(value)\n",
    "\n",
    "    def update(self, other):\n",
    "        # Needed because the default update method doesn't seem to use setitem\n",
    "        for k, v in other.items():\n",
    "            self[k] = v\n",
    "\n",
    "    def clean(self):\n",
    "        \"\"\"Sort the search results for each token by priority and un-overlap tokens.\"\"\"\n",
    "        for k, v in self.items():\n",
    "            _sort_search_results(v)\n",
    "        # unoverlap the keys\n",
    "        to_remove = []\n",
    "        for k in self:\n",
    "            to_search = self.keys() - {k}\n",
    "            if utils.is_overlap(k, to_search):\n",
    "                to_remove.append(k)\n",
    "        for i in to_remove:\n",
    "            del self[i]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = DFSearch(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entities: [Rating, Stewart, James Stewart, actor, James]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\05524X744\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Ignoring lemmatization.\n",
      "  \n",
      "C:\\Users\\05524X744\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: UserWarning: Cannot lemmatize multi-word cells.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '_search_groupby' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-578661a02b40>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0msearch_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-28-1510d838d670>\u001b[0m in \u001b[0;36msearch_args\u001b[1;34m(entities, args, lemmatized, fmt, argkeys)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[0msearch_res\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[0mentities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m     \u001b[0msearch_res\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_search_groupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlemmatized\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlemmatized\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m     \u001b[0msearch_res\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_search_sort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlemmatized\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlemmatized\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[0msearch_res\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_search_select\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlemmatized\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlemmatized\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name '_search_groupby' is not defined"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "# dfs.search(text)\n",
    "# dfs.search_nes(text)\n",
    "import numpy as np\n",
    "dfs.search(text)\n",
    "search_args(dfs.ents, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>name</th>\n",
       "      <th>rating</th>\n",
       "      <th>votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Actors</td>\n",
       "      <td>Humphrey Bogart</td>\n",
       "      <td>0.57</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Actors</td>\n",
       "      <td>Cary Grant</td>\n",
       "      <td>0.44</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Actors</td>\n",
       "      <td>James Stewart</td>\n",
       "      <td>0.99</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Actors</td>\n",
       "      <td>Marlon Brando</td>\n",
       "      <td>0.10</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Actors</td>\n",
       "      <td>Fred Astaire</td>\n",
       "      <td>0.21</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Actresses</td>\n",
       "      <td>Katharine Hepburn</td>\n",
       "      <td>0.04</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Actresses</td>\n",
       "      <td>Bette Davis</td>\n",
       "      <td>0.28</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Actresses</td>\n",
       "      <td>Audrey Hepburn</td>\n",
       "      <td>0.12</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Actresses</td>\n",
       "      <td>Ingrid Bergman</td>\n",
       "      <td>0.30</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Actors</td>\n",
       "      <td>Spencer Tracy</td>\n",
       "      <td>0.47</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Actors</td>\n",
       "      <td>Charlie Chaplin</td>\n",
       "      <td>0.24</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     category               name  rating  votes\n",
       "0      Actors    Humphrey Bogart    0.57    109\n",
       "1      Actors         Cary Grant    0.44    142\n",
       "2      Actors      James Stewart    0.99    120\n",
       "3      Actors      Marlon Brando    0.10    108\n",
       "4      Actors       Fred Astaire    0.21     84\n",
       "5   Actresses  Katharine Hepburn    0.04     63\n",
       "6   Actresses        Bette Davis    0.28     14\n",
       "7   Actresses     Audrey Hepburn    0.12     94\n",
       "8   Actresses     Ingrid Bergman    0.30     52\n",
       "9      Actors      Spencer Tracy    0.47    192\n",
       "10     Actors    Charlie Chaplin    0.24     76"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess_array_search(text, array, literal=False, case=False, lemmatize=True,\n",
    "                             nround=False):\n",
    "    nlp = utils.load_spacy_model()\n",
    "    if case or nround:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    if literal and lemmatize:\n",
    "        warnings.warn('Ignoring lemmatization.')\n",
    "\n",
    "    if not (literal or lemmatize):\n",
    "        warnings.warn(\n",
    "            'One of `literal` or `lemmatize` must be True. Falling back to lemmatize=True')\n",
    "        literal, lemmatize = False, True\n",
    "\n",
    "    if literal:  # ignore every other flag else\n",
    "        tokens = pd.Series([c.text for c in text], index=text)\n",
    "\n",
    "    elif lemmatize:\n",
    "        tokens = pd.Series([c.lemma_ for c in text], index=text)\n",
    "        if array.ndim == 1:\n",
    "            array = array.map(nlp)\n",
    "            array = pd.Series([token.lemma_ for doc in array for token in doc])\n",
    "        elif array.ndim == 2:\n",
    "            for col in array.columns[array.dtypes == np.dtype('O')]:\n",
    "                s = [c if isinstance(c, str) else str(c) for c in array[col]]\n",
    "                s = [nlp(c) for c in s]\n",
    "                try:\n",
    "                    array[col] = [token.lemma_ for doc in s for token in doc]\n",
    "                except ValueError:\n",
    "                    warnings.warn('Cannot lemmatize multi-word cells.')\n",
    "                    if not case:  # still need to respect the `case` param\n",
    "                        array[col] = array[col].str.lower()\n",
    "\n",
    "    return tokens, array\n",
    "\n",
    "def _search_1d_array(text, array, literal=False, case=False, lemmatize=True,\n",
    "                     nround=False):\n",
    "    tokens, array = _preprocess_array_search(text, array, literal, case, lemmatize, nround)\n",
    "    mask = array.isin(tokens)\n",
    "    if not mask.any():\n",
    "        return {}\n",
    "    if isinstance(mask, pd.Series):\n",
    "        nz = mask.to_numpy().nonzero()[0]\n",
    "    else:\n",
    "        nz = mask.nonzero()[0]\n",
    "    indices = {array[i]: i for i in nz}\n",
    "    tk = tokens[tokens.isin(array)]\n",
    "    return _remerge_span_tuples({token: indices[s] for token, s in tk.items()})\n",
    "\n",
    "def _search_2d_array(text, array, literal=False, case=False, lemmatize=True, nround=False):\n",
    "    array = array.astype(str)\n",
    "    tokens, array = _preprocess_array_search(text, array, literal, case, lemmatize, nround)\n",
    "    mask = array.isin(tokens.values)\n",
    "    if not mask.any().any():\n",
    "        return {}\n",
    "    indices = {array.iloc[i, j]: (i, j) for i, j in zip(*mask.values.nonzero())}\n",
    "    tk = tokens[tokens.isin(array.values.ravel())]\n",
    "    return _remerge_span_tuples({token: indices[s] for token, s in tk.items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remerge_span_tuples(results):\n",
    "    unmerged_spans = [k for k in results if isinstance(k, tuple)]\n",
    "    for span in unmerged_spans:\n",
    "        start, end = span[0].idx, span[-1].idx + len(span[-1])\n",
    "        new_span = span[0].doc.char_span(start, end)\n",
    "        results[new_span] = results.pop(span)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "NP_RULES = {\n",
    "    'NP1': [{'POS': 'PROPN', 'OP': '+'}],\n",
    "    'NP2': [{'POS': 'NOUN', 'OP': '+'}],\n",
    "    'NP3': [{'POS': 'ADV', 'OP': '+'}, {'POS': 'VERB', 'OP': '+'}],\n",
    "    'NP4': [{'POS': 'ADJ', 'OP': '+'}, {'POS': 'VERB', 'OP': '+'}],\n",
    "    'QUANT': [{'POS': 'NUM', 'OP': '+'}]\n",
    "}\n",
    "QUANT_PATTERN = re.compile(r'(^\\.d+|^d+\\.?(d?)+)')\n",
    "_spacy = {\n",
    "    'model': False,\n",
    "    'lemmatizer': False,\n",
    "    'matcher': False\n",
    "}\n",
    "\n",
    "def make_np_matcher(nlp, rules=NP_RULES):\n",
    "    \"\"\"Make a rule based noun phrase matcher.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nlp : `spacy.lang`\n",
    "        The spacy model to use.\n",
    "    rules : dict, optional\n",
    "        Mapping of rule IDS to spacy attribute patterns, such that each mapping\n",
    "        defines a noun phrase structure.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    `spacy.matcher.Matcher`\n",
    "    \"\"\"\n",
    "    if not _spacy['matcher']:\n",
    "        from spacy.matcher import Matcher\n",
    "        matcher = Matcher(nlp.vocab)\n",
    "        for k, v in rules.items():\n",
    "            matcher.add(k, None, v)\n",
    "        _spacy['matcher'] = matcher\n",
    "    else:\n",
    "        matcher = _spacy['matcher']\n",
    "    return matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "for k, v in NP_RULES.items():\n",
    "    matcher.add(k, None, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner(doc, matcher, match_ids=False, remove_overlap=True):\n",
    "    \"\"\"Find all NEs and other nouns in a spacy doc.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc: spacy.tokens.doc.Doc\n",
    "        The document in which to search for entities.\n",
    "    matcher: spacy.matcher.Matcher\n",
    "        The rule based matcher to use for finding noun phrases.\n",
    "    match_ids: list, optional\n",
    "        IDs from the spacy matcher to filter from the matches.\n",
    "    remove_overlap: bool, optional\n",
    "        Whether to remove overlapping tokens from the result.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List of spacy.token.span.Span objects.\n",
    "    \"\"\"\n",
    "    entities = set()\n",
    "    for span in doc.ents:\n",
    "        newtokens = [c for c in span if not c.is_space]\n",
    "        if newtokens:\n",
    "            newspan = doc[newtokens[0].i: (newtokens[-1].i + 1)]\n",
    "            entities.add(newspan)\n",
    "    if not match_ids:\n",
    "        entities.update([doc[start:end] for _, start, end in matcher(doc)])\n",
    "    else:\n",
    "        for m_id, start, end in matcher(doc):\n",
    "            if matcher.vocab.strings[m_id] in match_ids:\n",
    "                entities.add(doc[start:end])\n",
    "    if remove_overlap:\n",
    "        entities = unoverlap(entities)\n",
    "    return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unoverlap(tokens):\n",
    "    \"\"\"From a set of tokens, remove all tokens that are contained within\n",
    "    others.\"\"\"\n",
    "    textmap = {c: c for c in tokens}\n",
    "    newtokens = []\n",
    "    for token in tokens:\n",
    "        if not is_overlap(textmap[token], set(tokens) - {token}):\n",
    "            newtokens.append(token)\n",
    "    return [textmap[t] for t in newtokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ner() missing 2 required positional arguments: 'doc' and 'matcher'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-80be303dade6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: ner() missing 2 required positional arguments: 'doc' and 'matcher'"
     ]
    }
   ],
   "source": [
    "ner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[James, Stewart]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[c for c in doc.ents[0] if not c.is_space]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "James Stewart"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[newtokens[0].i: (newtokens[-1].i + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2603134184856246923, 0, 1),\n",
       " (2603134184856246923, 0, 2),\n",
       " (2603134184856246923, 1, 2),\n",
       " (4505603235458341185, 4, 5),\n",
       " (4505603235458341185, 8, 9)]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[James, James Stewart, Stewart, actor, Rating]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[doc[start:end] for _, start, end in matcher(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
